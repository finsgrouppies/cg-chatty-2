{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0705 16:53:40.451355   244 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "W0705 16:53:40.454747   244 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "W0705 16:53:40.467782   244 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0705 16:53:40.475803   244 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W0705 16:53:40.480257   244 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "W0705 16:53:40.481232   244 deprecation_wrapper.py:119] From c:\\program files\\python37\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# things we need for NLP\n",
    "# NLTK is a leading platform for building Python programs to work with human language data.\n",
    "# It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,\n",
    "# along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing,\n",
    "# and semantic reasoning.\n",
    "import nltk\n",
    "# A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# things we need for Tensorflow\n",
    "# Numpy is a library for the Python programming language, adding \n",
    "# support for large, multi-dimensional arrays and matrices, along with a \n",
    "# large collection of high-level mathematical functions to operate on these arrays.\n",
    "import numpy as np\n",
    "# TFlearn is a modular and transparent deep learning library built on top of Tensorflow.\n",
    "# It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations,\n",
    "# while remaining fully transparent and compatible with it.\n",
    "import tflearn\n",
    "# Tensorflow is an open source machine learning library, which is used to develop and train ML models\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "intent_file= open('intents.json','r')\n",
    "intents = json.load(intent_file)\n",
    "intent_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 documents\n",
      "26 classes ['soft_issues_response_errors', 'payments', 'goodbye', 'hours', 'bug_per', 'Ebanking', 'e_trans', 'greeting', 'services', 'EntSolutions', 'fins_trans', 'soft_issues_response_hard', 'finfinancials', 'soft_issues_response_soft_antivirus', 'thanks', 'ent_trans', 'soft_issues_response_noantivirus', 'opentoday', 'clearing_trans', 'get_services', 'departments', 'bug_resolve_affirmed', 'soft_issues', 'transaction', 'payments_n_clearing', 'get_info']\n",
      "103 unique stemmed words ['ebank', 'bye', 'bug', 'explain', 'hav', 'inform', 'is', 'neg', 'how', 'op', 'mor', 'softw', 'tel', 'provid', 'problem', 'card', 'do', 'cash', 'ther', 'er', 'yo', 'much', 'what', 'hi', 'structure', 'nop', 'about', 'program', 'solv', 'who', 'good', 'the', 'depart', 'hard', 'up', 'i', 'see', 'sort', 'exceiv', 'a', 'when', 'enterpr', 'expery', 'an', 'let', 'opt', ',', 'serv', 'credit', 'ok', 'deal', 'acceiv', 'no', 'in', 'finfin', 'yep', 'mastercard', 'posit', 'goodby', 'field', 'off', 'ar', 'fintech', 'it', 'you', 'perform', 'anyon', 'issu', 'thank', 'today', 'on', 'lat', 'am', 'consult', 'to', 'yeah', 'wil', 'help', '2', 'hello', \"'s\", 'pay', 'can', 'day', 'nee', 'hour', 'org', 'me', 'clear', 'settl', 'with', 'end', 'of', 'that', 'has', 'ye', 'read', 'out', 'keny', 'tak', 'know', 'enlight', 'for']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "references = []\n",
    "words_to_ignore = ['?','!']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    #note arr['name'] is visiting that array by name \n",
    "    for pattern in intent['question_patterns']:\n",
    "        # add to list containing the classes of words\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "        # tokenize each word in the sentence\n",
    "#Tokenization is the act of breaking up a sequence of strings into pieces such as words, keywords, phrases, symbols and other elements called tokens\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        # append word to our words list\n",
    "        words.extend(word)\n",
    "        # add to documents in our corpus \n",
    "#         A corpus is a collection of machine-readable texts that have been produced in a natural communicative setting\n",
    "#in the references multidim list of bot, append a list of words at index 0 and a list of contents of intents with key 'tag'\n",
    "        references.append((word, intent['tag']))\n",
    "        \n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [ stemmer.stem(word.lower()) for word in words if word not in words_to_ignore]\n",
    "#set method removes the duplicates in the words and classes obtained\n",
    "words = list(set(words))\n",
    "classes = list(set(classes))\n",
    "\n",
    "print (len(references), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data array bags\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "null_output = [0] * len(classes)\n",
    "#A vocabulary is a set of all words known to our bot\n",
    "# training set, vocabulary of words for each sentence\n",
    "#In the following method, we will convert the list of words we have to an array of tensors that tensorflow can use\n",
    "for ref in references:\n",
    "    # initialize our vocabulary of words\n",
    "    vocab = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = ref[0]\n",
    "    # stem each word, to its root(s) :-)\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our vocabulary of words array\n",
    "    for w in words:\n",
    "        vocab.append(1) if w in pattern_words else vocab.append(0)\n",
    "\n",
    "    # output is a '0' for inactive and '1' for active tag\n",
    "    output_row = list(null_output)\n",
    "    output_row[classes.index(ref[1])] = 1\n",
    "#append to multidim array training the vocabulary of root forms and the row they each correspond to\n",
    "    training.append([vocab, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "#shuffling will reduce the variance and overfitting\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "#the : stands for everything from the beginning to end , 0 represents the dimension, the 1st in 1d array\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "#print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 6299  | total loss: \u001b[1m\u001b[32m0.40221\u001b[0m\u001b[0m | time: 0.030s\n",
      "| Adam | epoch: 300 | loss: 0.40221 - acc: 0.7945 -- iter: 120/121\n",
      "Training Step: 6300  | total loss: \u001b[1m\u001b[32m0.41322\u001b[0m\u001b[0m | time: 0.032s\n",
      "| Adam | epoch: 300 | loss: 0.41322 - acc: 0.7984 -- iter: 121/121\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "tf.reset_default_graph()\n",
    "# Build neural network\n",
    "#shape will be 2D array of 1*length of a single train list\n",
    "input_data = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "#create fully connected layer with 36 neurons\n",
    "layer1 = tflearn.fully_connected(input_data,39,activation='ReLU')\n",
    "layer2 = tflearn.fully_connected(layer1,39,activation='relu6')\n",
    "net = tflearn.fully_connected(layer1, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model.fit(train_x, train_y, n_epoch=300, batch_size=6, show_metric=True,validation_batch_size=0.2)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all of our data structures\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trained_data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = trained_data['words']\n",
    "classes = trained_data['classes']\n",
    "train_x = trained_data['train_x']\n",
    "train_y = trained_data['train_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0705 16:54:03.702435   244 deprecation.py:323] From c:\\program files\\python37\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "model.load('./model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def make_vocab(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    #vocabulary of words\n",
    "    vocab = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                vocab[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "p = make_vocab(\"is your shop open today?\", words)\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR_THRESHOLD = 0.25\n",
    "# def classify(sentence):\n",
    "#     # generate probabilities from the model\n",
    "#     results = model.predict([unbag(sentence, words)])[0]\n",
    "#     # filter out predictions below a threshold\n",
    "#     results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "#     # sort by strength of probability\n",
    "#     results.sort(key=lambda x: x[1], reverse=True)\n",
    "#     return_list = []\n",
    "#     for r in results:\n",
    "#         return_list.append((classes[r[0]], r[1]))\n",
    "#     # return tuple of intent and probability\n",
    "#     return return_list\n",
    "\n",
    "# def response(sentence:str, userID='123', show_details=False):\n",
    "#     results = classify(sentence)\n",
    "#     # if we have a classification then find the matching intent tag\n",
    "#     if results:\n",
    "#         # loop as long as there are matches to process\n",
    "#         while results:\n",
    "#             for i in intents['intents']:\n",
    "#                 # find a tag matching the first result\n",
    "#                 if i['tag'] == results[0][0]:\n",
    "#                     # a random response from the intent\n",
    "#                     return str(random.choice(i['responses']))\n",
    "                \n",
    "\n",
    "#             results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify('is your shop open today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response('is your shop open today?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response('do you take cash?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response('what kind of mopeds do you rent?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response('Goodbye, see you later')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data structure to hold user context\n",
    "context = {}\n",
    "#minimum probability we accept that the bot is sure of what it says, in this case bot must be at least \n",
    "#10% sure before it replies\n",
    "ERROR_THRESHOLD = 0.10\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([make_vocab(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[k,v] for k,v in enumerate(results) if v>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability/descending order\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    classify_result = []\n",
    "    for result in results:\n",
    "        classify_result.append((classes[result[0]], result[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return classify_result\n",
    "\n",
    "def response(sentence, userID='user', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for intent in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if intent['tag'] == results[0][0]:\n",
    "                   #if the array json object contains a value of 'context_set', set that to the user session\n",
    "                    if 'context_set' in intent:\n",
    "                        if show_details: print ('context:', intent['context_set'])\n",
    "                        context[userID] = intent['context_set']\n",
    "\n",
    "                    # check if this intent is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in intent or \\\n",
    "                        (userID in context and 'context_filter' in intent and intent['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('tag:', intent['tag'])\n",
    "                        # a random response from the intent\n",
    "                        return str(random.choice(intent['responses']))\n",
    "\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Click on start then move to control panel, are you there?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('we want to rent a moped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice, now click overclock, and then finfinancials. Clear the cache and agree to reboot'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('yeah')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We're open every day from 9am-9pm\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our hours are 9am-9pm every day'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('today please')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': 'per_troubleshoot'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: \n",
      "tag: greeting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello, thanks for visiting Fintech'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response(\"Hi there!\", show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We're open every day from 9am-9pm\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('opentoday', 0.71860814), ('soft_issues_response_noantivirus', 0.12539037)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, thanks for visiting Fintech'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'does the log message displayed contain HARD or SOFT at the ending?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response('i have some erors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finfinancials', 0.9940521)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(\"explain finfinancials?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user': 'error_finder'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0705 16:56:53.276874   244 _internal.py:122]  * Running on http://localhost:9000/ (Press CTRL+C to quit)\n",
      "I0705 16:56:55.726320   244 _internal.py:122] 127.0.0.1 - - [05/Jul/2019 16:56:55] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "I0705 16:56:58.736612   244 _internal.py:122] 127.0.0.1 - - [05/Jul/2019 16:56:58] \"\u001b[37mPOST /process HTTP/1.1\u001b[0m\" 200 -\n",
      "I0705 16:57:01.068172   244 _internal.py:122] 127.0.0.1 - - [05/Jul/2019 16:57:01] \"\u001b[37mPOST /process HTTP/1.1\u001b[0m\" 200 -\n",
      "I0705 16:57:01.368215   244 _internal.py:122] 127.0.0.1 - - [05/Jul/2019 16:57:01] \"\u001b[37mPOST /process HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Flask is a micro framework for Python aiming to provide the flexibility of creating web sites and APIs.\n",
    "from flask import Flask, request,render_template\n",
    "app = Flask(__name__)\n",
    "app.static_folder = 'static'\n",
    "@app.route('/')\n",
    "def index():\n",
    "\treturn render_template('index.html')\n",
    "@app.route('/process',methods=['POST'])\n",
    "def process():\n",
    "\tuser_input=request.form['user_input']\n",
    "\tbot_response=response(user_input)\n",
    "\tif(bot_response is None):\n",
    "\t\tbot_response=random.choice(['Sorry, I fail to understand that, Im a bot','Please rephrase the question, im just a bot'])\n",
    "    \n",
    "\n",
    "\n",
    "\t\n",
    "\treturn render_template('index.html',user_input=user_input,bot_response=bot_response)\n",
    "#make the session last longer as jupyter does not provide the ability\n",
    "from werkzeug.serving import run_simple\n",
    "run_simple('localhost', 9000, app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
